# -*- coding: utf-8 -*-
"""bidbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MQFbbk3ZpjcbETbnIJAI50UbwsaxfyaJ
"""

# format yyyymmdd (輸入起始日期)
START_DATE= 20241118

# year conversion
minguoSearchYear = (START_DATE//10000)-1911
# binary classification threshold
CLASSIFICATION_THRESHOLD = 0.0

"""# 參數設定"""

# imports
import json
from transformers import pipeline
import pandas as pd
import re
import requests
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime
import pytz

config = {
    "keywords": {
        "by_title": [
            "細胞", "菌株", "盤式", "微盤", "微孔盤", "微量盤", "微量孔盤",
            "影像", "螢光", "冷光", "分光", "光譜", "全波長", "高內涵",
            "吸光", "光學分析", "連續波長", "酵素免疫", "清洗機", "清洗系統", "共軛焦"
        ],
        "by_company": [
            "進階生物", "奇異", "尚博", "博克", "岑祥股份", "金萬林企業", "科羅耐",
            "尖端先進", "倍晶生技", "豐記", "科光", "正茂", "冷泉港", "侒欣", "慧禾", "萊富"
        ]
    },
    "websites":{
        "pcc_gov": {
            "name": "政府採購網",
            "url": ""
        },
        "g0v": {
            "name": "g0v政府採購網api",
            "api": {
                "base_url": "https://pcc.g0v.ronny.tw/api/",
                "endpoints": {
                    "search_title": "searchbytitle?",
                    "search_company": "searchbycompanyname?",
                    "tenders": "tender?"
                }
            }
        },
        "dcb": {
            "name": "生技中心網頁採購資訊",
            "url": "https://www.dcb.org.tw/purchase_demo?locale=zh-TW"
        },
        "sinica": {
            "name": "中研採購網",
            "tender_url": "https://srp.sinica.edu.tw/#/tender/inst/00/page/1",
            "awarding_url": "https://srp.sinica.edu.tw/#/awarding/inst/00/page/1",
            "failbids_url": "https://srp.sinica.edu.tw/#/failbids/inst/00/page/1"
        }
    },
    "data_to_collect": {
        "pcc_gov": {
            "tender_columns": [
                "date", "brief.title", "brief.category", "job_number",
                "detail.機關資料:機關名稱", "detail.機關資料:單位名稱",
                "detail.領投開標:截止投標", "detail.招標資料:招標狀態",
                "detail.機關資料:聯絡人", "detail.機關資料:聯絡電話",
                "detail.機關資料:電子郵件信箱", "detail.機關資料:傳真號碼",
                "detail.採購資料:標的分類", "detail.採購資料:預算金額", "detail.url"
            ],
            "award_columns": [
                "date", "brief.title", "brief.type", "job_number",
                "detail.機關資料:機關名稱", "detail.機關資料:單位名稱",
                "detail.機關資料:聯絡人", "detail.機關資料:聯絡電話",
                "detail.機關資料:電子郵件信箱", "detail.機關資料:傳真號碼",
                "detail.決標品項:第1品項:品項名稱", "detail.決標品項:第1品項:得標廠商1:得標廠商",
                "detail.決標資料:總決標金額", "detail.採購資料:標的分類", "detail.url"
            ],
            "not_award_columns": [
                "date", "brief.title", "brief.type", "job_number",
                "detail.無法決標公告:機關名稱", "detail.無法決標公告:單位名稱",
                "detail.無法決標公告:無法決標的理由", "detail.無法決標公告:聯絡人",
                "detail.無法決標公告:聯絡電話", "detail.無法決標公告:電子郵件信箱",
                "detail.無法決標公告:傳真號碼", "detail.無法決標公告:標的分類", "detail.url"
            ]
        }
    }
}

# Keywords
TITLE_KEYWORDS = config['keywords']['by_title']
COMPANY_KEYWORDS = config['keywords']['by_company']

# Search Methods
API_BASE_URL = config['websites']['g0v']['api']['base_url']
API_BY_TITLE = config['websites']['g0v']['api']['endpoints']['search_title']
API_BY_COMPANY = config['websites']['g0v']['api']['endpoints']['search_company']

# Record Methods
TENDER_SELECTED_COLUMNS = config["data_to_collect"]["pcc_gov"]["tender_columns"]
AWARD_SELECTED_COLUMNS = config["data_to_collect"]["pcc_gov"]["award_columns"]
NOT_AWARD_SELECTED_COLUMNS = config["data_to_collect"]["pcc_gov"]["not_award_columns"]

"""# g0v api 資料下載 new_tender_df, new_award_df"""

def request_json(api):
    response = requests.get(api)
    if response.status_code == 200:
        json_data = response.json()
        return json_data
    else:
        print(f"Failed to fetch data. Status code: {response.status_code}")
        return None

def write_to_df(record, record_type):
    filtered_dict = {}
    for key in record_type:

        # Access the nested value if nested
        if "." in key:
            nested_keys = key.split(".")
            nested_value = record
            for nested_key in nested_keys:
                if nested_key in nested_value:
                    nested_value = nested_value[nested_key]
                else:
                    nested_value = None
                    break
            filtered_dict[nested_key] = nested_value
        elif key in record:
            filtered_dict[key] = record[key]

    df = pd.DataFrame([filtered_dict])
    return df
def write_to_df(record, record_type):
    filtered_dict = {}
    for key in record_type:

        # Access the nested value if nested
        if "." in key:
            nested_keys = key.split(".")
            nested_value = record
            for nested_key in nested_keys:
                if nested_key in nested_value:
                    nested_value = nested_value[nested_key]
                else:
                    nested_value = None
                    break
            filtered_dict[nested_key] = nested_value
        elif key in record:
            filtered_dict[key] = record[key]

    df = pd.DataFrame([filtered_dict])
    return df

def search_api(keyword, method, date):
    full_records = pd.DataFrame()
    endpoint = method
    query = quote(f'{keyword}')
    page = 1

    api = f'{API_BASE_URL}{endpoint}query={query}&page={page}'
    json_data = request_json(api)
    raw_records_df = pd.DataFrame(json_data['records'])
    records_df = raw_records_df[['date', 'filename', 'brief', 'tender_api_url']]
    filtered_df = records_df[records_df['date'] >= date]
    full_records = pd.concat([full_records, filtered_df], ignore_index=True)
    # next page
    while len(filtered_df) == 100:
      page += 1
      api = f'{API_BASE_URL}{endpoint}query={query}&page={page}'
      json_data = request_json(api)
      raw_records_df = pd.DataFrame(json_data['records'])
      records_df = raw_records_df[['date', 'filename', 'brief', 'tender_api_url']]
      filtered_df = records_df[records_df['date'] >= date]
      full_records = pd.concat([full_records, filtered_df], ignore_index=True)

    full_records['tender_type'] = full_records['brief'].apply(lambda x: x['type'])
    del full_records['brief']
    valid_tender_types = ['無法決標公告', '決標公告', '公開招標公告','公開取得報價單或企劃書公告','公開取得報價單或企劃書更正公告','經公開評選或公開徵求之限制性招標公告']
    full_records = full_records[full_records['tender_type'].isin(valid_tender_types)]

    return full_records

def scrape_data(record_df):
    tenders_df = pd.DataFrame()
    awards_df = pd.DataFrame()

    for index, row in record_df.iterrows():
        api_url = row['tender_api_url']
        filename = row['filename']
        json_data = request_json(api_url)

        # find matching record with filename
        records = json_data.get('records', [])
        matching_record = next((record for record in records if record.get('filename', '') == filename), None)

        if row['tender_type'] == '決標公告':
            award_df = write_to_df(matching_record, AWARD_SELECTED_COLUMNS)
            awards_df = pd.concat([awards_df, award_df], ignore_index=True)

        elif row['tender_type'] == '無法決標公告':
            award_df = write_to_df(matching_record, NOT_AWARD_SELECTED_COLUMNS)
            awards_df = pd.concat([awards_df, award_df], ignore_index=True)

        else:
            tender_df = write_to_df(matching_record, TENDER_SELECTED_COLUMNS)
            tenders_df = pd.concat([tenders_df, tender_df], ignore_index=True)

    return tenders_df, awards_df

new_tender_df = pd.DataFrame()
new_award_df = pd.DataFrame()


# search with title
for keyword in TITLE_KEYWORDS:
    print(keyword)

    title_search_df = search_api(keyword, API_BY_TITLE, START_DATE)
    tender_df, award_df = scrape_data(title_search_df)

    tender_df.insert(0, 'keyword', keyword)
    award_df.insert(0, 'keyword', keyword)
    new_tender_df = pd.concat([new_tender_df, tender_df], ignore_index=True)
    new_award_df = pd.concat([new_award_df, award_df], ignore_index=True)

new_tender_df = new_tender_df.rename(columns={'機關資料:機關名稱': '機關名稱'})

# search with company
for keyword in COMPANY_KEYWORDS:
    print(keyword)

    company_search_df = search_api(keyword, API_BY_COMPANY, START_DATE)
    tender_df, award_df = scrape_data(company_search_df)
    tender_df.insert(0, 'keyword', keyword)
    award_df.insert(0, 'keyword', keyword)
    new_tender_df = pd.concat([new_tender_df, tender_df], ignore_index=True)
    new_award_df = pd.concat([new_award_df, award_df], ignore_index=True)

new_tender_df

# Award table formatting
# Define the column pairs for combination
column_pairs = [
    ('機關名稱', '機關資料:機關名稱', '無法決標公告:機關名稱'),
    ('單位名稱', '機關資料:單位名稱', '無法決標公告:單位名稱'),
    ('聯絡人', '機關資料:聯絡人', '無法決標公告:聯絡人'),
    ('聯絡電話', '機關資料:聯絡電話', '無法決標公告:聯絡電話'),
    ('電子郵件信箱', '機關資料:電子郵件信箱', '無法決標公告:電子郵件信箱'),
    ('傳真號碼', '機關資料:傳真號碼', '無法決標公告:傳真號碼'),
    ('標的分類', '採購資料:標的分類', '無法決標公告:標的分類')
]

for new_col, col1, col2 in column_pairs:
    if col1 in new_award_df.columns and col2 in new_award_df.columns:
        new_award_df[new_col] = new_award_df[col1].combine_first(new_award_df[col2])
    elif col1 in new_award_df.columns:
        new_award_df[new_col] = new_award_df[col1]
    elif col2 in new_award_df.columns:
        new_award_df[new_col] = new_award_df[col2]

# Check for '已公告資料:標的分類' separately since it is only used in the last step
if '已公告資料:標的分類' in new_award_df.columns:
    new_award_df['標的分類'] = new_award_df['標的分類'].combine_first(new_award_df['已公告資料:標的分類'])

# Drop only the columns that exist in the DataFrame
columns_to_drop = ['機關資料:機關名稱', '無法決標公告:機關名稱', '機關資料:單位名稱', '無法決標公告:單位名稱',
                   '機關資料:聯絡人', '無法決標公告:聯絡人', '機關資料:聯絡電話', '無法決標公告:聯絡電話',
                   '機關資料:電子郵件信箱', '無法決標公告:電子郵件信箱', '機關資料:傳真號碼',
                   '無法決標公告:傳真號碼', '採購資料:標的分類', '已公告資料:標的分類', '無法決標公告:標的分類']

# Use set intersection to find only columns that exist in the DataFrame
existing_columns_to_drop = [col for col in columns_to_drop if col in new_award_df.columns]

# Drop the existing columns
new_award_df.drop(existing_columns_to_drop, axis=1, inplace=True)


# New tender標的分類
values_to_keep = ['財物類352-醫藥產品', '財物類481-醫療,外科及矯形設備', '財物類482-做為測量、檢查、航行及其他目的用之儀器和裝置，除光學儀器;工業程序控制設備;上述各項之零件及附件', '財物類483-光學儀器,攝影設備及其零件與附件', '財物類449-其他特殊用途之機具及其零件']

if '採購資料:標的分類' in new_tender_df.columns:
    new_tender_df = new_tender_df.drop(new_tender_df.index[~new_tender_df['採購資料:標的分類'].isin(values_to_keep)])

"""# 政府採購網對照組下載 gov_tender, gov_award"""

gov_search_df = pd.DataFrame()
baseUrl = "https://web.pcc.gov.tw/prkms/tender/common/bulletion/readBulletion"
href_baseUrl = 'https://web.pcc.gov.tw/'





#move to module
def request_html(api):
    response = requests.get(api)
    if response.status_code == 200:
        html_data = response.text
        return html_data
    else:
        print(f"Failed to fetch data. Status code: {response.status_code}")
        return None


gov_tender=pd.DataFrame()
gov_award=pd.DataFrame()


for keyword in TITLE_KEYWORDS:
  print(keyword)

  # Tender search result
  tenderQueryStrings = "?querySentence=" + keyword + "&tenderStatusType=%E6%8B%9B%E6%A8%99&sortCol=TENDER_NOTICE_DATE&timeRange=" + str(minguoSearchYear) + "&pageSize=100"

  while True:
    html = request_html(baseUrl + tenderQueryStrings)
    soup = BeautifulSoup(html, 'html')
    tbody = soup.find("tbody")

    # extract data
    pattern = r'Geps3\.CNS\.pageCode2Img\("([^"]*)"\)'
    tender_names = re.findall(pattern, html)


    organizations = []
    job_numbers = []
    hyperlinks = []
    tender_dates = []
    award_dates = []
    tds = tbody.find_all("td")
    i = 0
    for td in tds:
      if i % 10 == 2:
        org = td.get_text(strip=True)
        organizations.append(org)
      elif i % 10 == 3:
        href = td.find('a')
        hyperlink = href['href']
        href = href_baseUrl + hyperlink
        hyperlinks.append(href)
        job_num = td.get_text(strip=True)
        job_numbers.append(job_num)
      elif i % 10 == 4:
        date = td.get_text(strip=True)
        roc_year, month, day = date.split('/')
        ce_year = str(int(roc_year) + 1911)
        formatted_date = ce_year + month + day
        tender_dates.append(formatted_date)
      elif i % 10 == 6:
        date = td.get_text(strip=True)
        award_dates.append(date)
      i += 1

    raw_gov_tender = pd.DataFrame({'date' : tender_dates, 'title' : tender_names,
                                  'keyword' : keyword, 'job_number' : job_numbers, '機關名稱' : organizations, '領投開標:截止投標': award_dates, 'url': hyperlinks} )

    raw_gov_tender['date'] = raw_gov_tender['date'].astype(int)
    df1 = raw_gov_tender[raw_gov_tender['date'] >= START_DATE]
    gov_tender = pd.concat([gov_tender,df1],ignore_index=True)

    break


    '''row_count = len(df1)
    if row_count < 100:
      break

    else:
      # Find the 'a' element with text "下一頁"
      next_page_link = soup.find('a', string='下一頁')
      # Extract the 'href' attribute
      if next_page_link is not None:
        tenderQueryStrings = next_page_link.get('href')
      else:
        print("next page not found")'''




  # Award search result
  awardQueryStrings = "?querySentence=" + keyword + "&tenderStatusType=%E6%B1%BA%E6%A8%99&sortCol=AWARD_NOTICE_DATE&timeRange=" + str(minguoSearchYear) + "&pageSize=100"
  html = request_html(baseUrl + awardQueryStrings)
  soup = BeautifulSoup(html, 'html')
  tbody = soup.find("tbody")

  # extract data
  pattern = r'Geps3\.CNS\.pageCode2Img\("([^"]*)"\)'
  award_names = re.findall(pattern, html)

  organizations = []
  job_numbers = []
  hyperlinks = []
  award_dates = []
  award_types = []
  tds = tbody.find_all("td")
  i = 0
  for td in tds:
    if i % 10 == 2:
      org = td.get_text(strip=True)
      organizations.append(org)
    elif i % 10 == 3:
      href = td.find('a')
      hyperlink = href['href']
      href = href_baseUrl + hyperlink
      hyperlinks.append(href)
      job_num = td.get_text(strip=True)
      job_numbers.append(job_num)
    elif i % 10 == 5:
      td_date = td.get_text(strip=True)
      date = re.search(r'\d+/\d+/\d+', td_date).group()
      roc_year, month, day = date.split('/')
      ce_year = str(int(roc_year) + 1911)
      formatted_date = ce_year + month + day
      award_dates.append(formatted_date)
      award_given = td.find('span')
      if award_given:
        award_type = '無法決標公告'
      else:
        award_type = '決標公告'
      award_types.append(award_type)
    i += 1

  raw_gov_award = pd.DataFrame({'date' : award_dates, 'title' : award_names,
                                 'keyword' : keyword, 'type' : award_types, 'job_number' : job_numbers, '機關名稱' : organizations, 'url': hyperlinks} )
  raw_gov_award['date'] = raw_gov_award['date'].astype(int)
  df2 = raw_gov_award[raw_gov_award['date'] >= START_DATE]
  gov_award = pd.concat([gov_award,df2],ignore_index=True)



# Company keyword
for keyword in COMPANY_KEYWORDS:
  print(keyword)
  awardQueryStrings = "?querySentence=" + keyword + "&tenderStatusType=%E6%B1%BA%E6%A8%99&sortCol=AWARD_NOTICE_DATE&timeRange=" + str(minguoSearchYear) + "&pageSize=100"

  html = request_html(baseUrl + awardQueryStrings)
  soup = BeautifulSoup(html, 'html')
  tbody = soup.find("tbody")

  # extract data
  pattern = r'Geps3\.CNS\.pageCode2Img\("([^"]*)"\)'
  award_names = re.findall(pattern, html)

  organizations = []
  job_numbers = []
  hyperlinks = []
  award_dates = []
  award_types = []
  tds = tbody.find_all("td")
  i = 0
  for td in tds:
    if i % 10 == 2:
      org = td.get_text(strip=True)
      organizations.append(org)
    elif i % 10 == 3:
      href = td.find('a')
      hyperlink = href['href']
      href = href_baseUrl + hyperlink
      hyperlinks.append(href)
      job_num = td.get_text(strip=True)
      job_numbers.append(job_num)
    elif i % 10 == 5:
      td_date = td.get_text(strip=True)
      date = re.search(r'\d+/\d+/\d+', td_date).group()
      roc_year, month, day = date.split('/')
      ce_year = str(int(roc_year) + 1911)
      formatted_date = ce_year + month + day
      award_dates.append(formatted_date)
      award_given = td.find('span')
      if award_given:
        award_type = '無法決標公告'
      else:
        award_type = '決標公告'
      award_types.append(award_type)
    i += 1

  raw_gov_award = pd.DataFrame({'date' : award_dates, 'title' : award_names,
                                 'keyword' : keyword, 'type' : award_types, 'job_number' : job_numbers, '機關名稱' : organizations, 'url': hyperlinks} )
  raw_gov_award['date'] = raw_gov_award['date'].astype(int)
  df2 = raw_gov_award[raw_gov_award['date'] >= START_DATE]
  gov_award = pd.concat([gov_award,df2],ignore_index=True)

"""#Combine g0v api data with 政府採購網"""

def remove_spaces_around_chinese(text):
    # Define a pattern to match Chinese characters
    chinese_char_pattern = re.compile(r'[\u4e00-\u9fff]')

    # Split the text by spaces
    parts = text.split()

    # Remove spaces around Chinese characters
    cleaned_parts = []
    for part in parts:

        if cleaned_parts and chinese_char_pattern.search(part):
            cleaned_parts[-1] += part

        elif cleaned_parts and chinese_char_pattern.search(cleaned_parts[-1][-1]):
                # Append English parts without space if previous part was Chinese
            cleaned_parts[-1] += part

        else:
            cleaned_parts.append(part)

    # Join the cleaned parts with spaces
    cleaned_text = ' '.join(cleaned_parts)

    return cleaned_text

'''txt = "正立螢光電動顯微觀察系統 如CARL ZEISS AXIO IMAGER M2 或同等品以上 詳規範 共1ST"
remove_spaces_around_chinese(txt)'''

# Remove spaces between chinese characters, for gov source comparison
if 'title' in new_tender_df.columns:
  new_tender_df['title'] = new_tender_df['title'].apply(remove_spaces_around_chinese)
if 'title' in new_award_df.columns:
  new_award_df['title'] = new_award_df['title'].apply(remove_spaces_around_chinese)

#Check empty dataframe
empty_tender_df = pd.DataFrame(columns=gov_tender.columns)
empty_award_df = pd.DataFrame(columns=gov_award.columns)
new_tender_df = new_tender_df if not new_tender_df.empty else empty_tender_df
new_award_df = new_award_df if not new_award_df.empty else empty_award_df

# Outer merge gov and g0v data
merged_tender_df = gov_tender.merge(new_tender_df, on=['date', 'title','keyword', 'job_number', '機關名稱', '領投開標:截止投標', 'url'], how='outer')
merged_award_df = gov_award.merge(new_award_df, on=['date', 'title', 'keyword', 'type', 'job_number', '機關名稱', 'url'], how='outer')

# Need to delete duplicates after merging

"""# Classification Model"""

pipe = pipeline("text-classification", model = 'manatiw/test-trainer-imbalanced1.0')

def predict(value):
  prediction = pipe(value)
  correlation = prediction[0]['label']
  score = prediction[0]['score']
  if correlation == 'negative':
    pos_score = (1 - score)
  else:
    pos_score = score
  return pos_score


merged_tender_df['score'] = merged_tender_df['title'].apply(predict)
merged_award_df['score'] = merged_award_df['title'].apply(predict)

"""#Data processing"""

# Sort by date
merged_tender_df = merged_tender_df.sort_values(by='date')
merged_award_df = merged_award_df.sort_values(by='date')

# Delete Duplicates(drop
merged_tender_df['missing_values_count'] = merged_tender_df.isnull().sum(axis=1) + merged_tender_df.apply(lambda x: x == '', axis=1).sum(axis=1)
merged_award_df['missing_values_count'] = merged_award_df.isnull().sum(axis=1) + merged_award_df.apply(lambda x: x == '', axis=1).sum(axis=1)
merged_tender_df = merged_tender_df.sort_values(by=['date', 'job_number', 'missing_values_count'])
merged_award_df = merged_award_df.sort_values(by=['date', 'job_number', 'missing_values_count'])
cleaned_tender_df = merged_tender_df.drop_duplicates(subset=['date', 'job_number'], keep='first')
cleaned_award_df = merged_award_df.drop_duplicates(subset=['date', 'job_number'], keep='first')
cleaned_tender_df = cleaned_tender_df.drop(columns=['missing_values_count'])
cleaned_award_df = cleaned_award_df.drop(columns=['missing_values_count'])


# Apply threshold
merged_tender_df = merged_tender_df[merged_tender_df['score'] >= CLASSIFICATION_THRESHOLD]
merged_award_df = merged_award_df[merged_award_df['score'] >= CLASSIFICATION_THRESHOLD]

"""if duplicate fails to drop"""

#merged_tender_df.replace('', pd.NA, inplace=True)
#merged_tender_df['date'] = merged_tender_df['date'].astype(str)
#merged_tender_df['job_number'] = merged_tender_df['job_number'].astype(str)
#merged_tender_df['date'] = merged_tender_df['date'].str.strip()
#merged_tender_df['job_number'] = merged_tender_df['job_number'].str.strip()

"""# Save Results"""

today_str = datetime.today().strftime('%m%d%Y')
tz = pytz.timezone('Asia/Taipei')
today_str = datetime.now(tz).strftime('%m%d%Y')
cleaned_tender_df.to_csv(f'{today_str}newTender.csv', index=False, encoding='utf-8-sig')
cleaned_award_df.to_csv(f'{today_str}newAward.csv', index=False, encoding='utf-8-sig')